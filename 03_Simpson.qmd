---
title: "Curso de Fatores"
subtitle: "*Paradoxo de Simpson e Imputação de Dados*"
author: "Luiz Droubi"
institute: Academia da Engenharia de Avaliações
date: last-modified
date-format: long
title-slide-attributes:
  data-background-image: img/PPT_abertura.png
  #data-background-size: contain
  #data-background-opacity: "0.5"
  data-footer: "<a href='http://www.valoristica.com.br'>http://www.valoristica.com.br</a>"
include-after-body: add-custom-footer.html
lang: pt
format:
  revealjs:
    theme: [default, style.scss]
    logo: img/logo.png
    # theme: beige
    # smaller: true
    scrollable: true
    incremental: true
    transition: slide
    background-transition: fade
fontsize: 2em
bibliography: references.bib
brand: true
toc: true
toc-depth: 1
footer: "VALORÍSTICA"
slide-number: true
---

```{r}
#| include: false
library(kableExtra)
library(appraiseR)
library(broom)
library(mice)
library(ggplot2)
library(ggthemes)
library(ggpmisc)
library(ggmice)
```

```{r}
library(appraiseR)
library(MASS)

# Data parameters
n <- 30
m <- c(450, 15, 4, 2500)
s <- c(175, 4, 3, 250)
ryx1 <- -0.50; ryx2 <- -0.30; ryx3 <- -.40; rx1x2 <- .90; rx1x3 <- 0; rx2x3 <- 0

sigma <- sqrt(log(s^2/m^2 + 1))
mu <- log(m) - sigma^2/2
rhoyx1 <- log(ryx1*prod(s[c(4, 1)])   / prod(m[c(4, 1)]) + 1)
rhoyx2 <- log(ryx2*prod(s[c(4, 2)])   / prod(m[c(4, 2)]) + 1)
rhoyx3 <- log(ryx3*prod(s[c(4, 3)])   / prod(m[c(4, 3)]) + 1)
rhox1x2 <- log(rx1x2*prod(s[1:2])     / prod(m[1:2]) + 1)
rhox1x3 <- log(rx1x3*prod(s[c(1, 3)]) / prod(m[c(1, 3)]) + 1)
rhox2x3 <- log(rx2x3*prod(s[2:3]) / prod(m[2:3]) + 1)

##                 Area         Frente        Incl.         #PU
Sigma <- matrix(c(sigma[1]^2,    rhox1x2,    rhox1x3,     rhoyx1,  # Area
                     rhox1x2, sigma[2]^2,    rhox2x3,     rhoyx2,  # Frente
                     rhox1x3,    rhox2x3, sigma[3]^2,     rhoyx3,  # Incl.
                      rhoyx1,     rhoyx2,     rhoyx3, sigma[4]^2   # PU
                  ),         # Frente
                ncol = 4, byrow = T)

# Create data
set.seed(1)
dados <- exp(mvrnorm(n=n, mu=  mu, Sigma = Sigma,
                     empirical = T))
colnames(dados) <- c("Area", "Frente", "Incl", "PU")
dados <- apply(dados, 2, round, 2)
dados <- as.data.frame(dados)
```

```{r}
# Impute NA's
set.seed(2)
dados$Incl[sample(1:25, 10)] <- NA
dados$Frente[sample(1:25, 5)] <- NA

# Adjust fit
fit <- lm(log(PU) ~ log(Area) + log(Frente) + log(Incl), data = dados)
AreaCoef <- coef(fit)["log(Area)"]
FrenteCoef <- coef(fit)["log(Frente)"]
```

# Estudo de Caso {background-image="img/PPT_Chapter.png"}  

## Dados

- Numa amostra de dados de mercado obtivemos:
  - 30 dados de terrenos de variadas características
  - Algumas variáveis foram coletadas de forma incompleta, como a variável 
  `Frente` e a variável `Incl` (inclinação da superfície do terreno).
  
## Dados

```{r}
library(vtable)
st(dados, out = "kable", add.median = T)
```

- Nota-se que existem apenas 25 dados com a variável `Frente`.
- E que existem apenas 20 dados com a variável `Incl`.

## Modelo de Regressão Múltipla {.smaller}

### Apenas casos completos

```{r}
library(broom)
library(knitr)
library(kableExtra)
s <- summary(fit)
fit |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2, format.args = list(big.mark = ".", decimal.mark = ","),
        format = "html", 
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",   
                      "Inf.", "Sup.")) |>
  footnote(alphabet = c(paste("Dados: ", length(fit$residuals)), 
                        paste("R2: ", brf(s$r.squared, digits = 2)),
                        paste("R2aj: ", brf(s$adj.r.squared, digits = 2))
                        ),
           escape = F)  |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

- Variável `Frente` não se mostrou significante!

- Por contar com apenas 15 dados completos, a estimação ficou prejudicada!

# Análise exploratória {background-image="img/PPT_Chapter.png"}  

## Variável Área

```{r}
p1 <- 
  ggplot(dados, aes(x = Area/450, y = PU)) +
  geom_point(color = "cornflowerblue", size = 2) +
  stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/450) + k,
              method.args=list(start=c(a=-100, k=10000)))
p1
```

## Variável Frente

```{r}
library(mice)
library(ggmice)
p2 <-
  ggmice(dados, aes(x = Frente/15, y = PU)) +
  geom_point(size = 2) +
  stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/15) + k,
              method.args=list(start=c(a=-100, k=10000)))  +
  theme_solarized(base_size = 15)
p2
```

## Variável Inclinação

```{r}
formula <- y ~ a*log1p(x) + k
p3 <- 
  ggmice(dados, aes(x = Incl, y = PU)) +
  geom_point(size = 2) +
  # stat_poly_line(formula = formula)
  stat_smooth(method="nls", se=FALSE, formula=y~a*log1p(x) + k,
              method.args=list(start=c(a=-100, k=10000)))  +
  theme_solarized(base_size = 15)
p3
```

# Derivação de Fatores {background-image="img/PPT_Chapter.png"} 

## Fator Área {.smaller}

```{r}
library(ggpmisc)
old.option <- options(OutDec = ",")
p1 +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P"), sep = "*\"; \"*"), 
               formula = log(y) ~ log(x),
               coef.digits = 2, coef.keep.zeros = FALSE, 
               f.digits = 3, p.digits = 2,
               eq.with.lhs = "widehat(ln(PU))~`=`~",
               eq.x.rhs = ".ln(Area/450)",
               label.x = "left", label.y = "bottom", color = "red",
               size = 5)
```

- Dado que a área tem correlação com a variável PU, na forma log-log, pode-se assim ajustar um fator área:

- $$F_a = \left ( \frac{A_{imovel}}{A_{paradigma}} \right)^{-0,15} = \left ( \frac{A_{paradigma}}{A_{imovel}} \right)^{0,15} = \left ( \frac{450}{A_{imovel}} \right)^{0,15}$$

## Fator Área (2) {.smaller}

```{r}
#| echo: true
fitArea <- lm(log(PU) ~ log(Area/450), data = dados)
```

```{r}
fitArea |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2,
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",                        "Inf.", "Sup.")) |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

- De acordo com o modelo acima, um imóvel paradigma (`Area` = 450m2), tem VM de:

. . .

```{r}
#| echo: true
#predict(fitArea, newdata = list(Area = 450))
exp(7.81)
```

- Já para um imóvel de 750m2, tem-se: $F_{Area} = (450/750)^{0,15} = 0,926$

- Para avaliar o valor de mercado do lote de 750 m2: $E[PU|A=750] = 0,926.2.465=2282,60 \text{ R\$/m}^2$

. . .

```{r}
#| echo: true
#predict(fitArea, newdata = list(Area = 450))
p <- predict(fitArea, newdata = list(Area = 750))
exp(p)
```

## Fator Frente {.smaller}

```{r}
p2 +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P"), sep = "*\"; \"*"), 
               formula = log(y) ~ log(x), 
               coef.digits = 2, coef.keep.zeros = FALSE, 
               f.digits = 2, p.digits = 2,
               eq.with.lhs = "widehat(ln(PU))~`=`~",
               eq.x.rhs = ".ln(Frente/15)",
               label.x = "left", label.y = "bottom", color = "red",
               size = 5)
```

- Dado que não há evidência forte da correlação entre as variáveis `PU` e `Frente`, pode-se concluir que a variável `Frente` não é estatisticamente significante e, portanto, não é necessário o ajuste de um fator frente!

- Além do mais, um fator frente assim ajustado seria contraditório: quanto maior a frente, menores os preços unitários!

## Fator inclinação {.smaller}

```{r}
p3 +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P"), sep = "*\"; \"*"), 
               formula = log(y) ~ log1p(x), 
               coef.digits = 3, coef.keep.zeros = FALSE, 
               f.digits = 2, p.digits = 2,
               eq.with.lhs = "widehat(ln(PU))~`=`~",
               eq.x.rhs = ".ln(Incl+1)",
               label.x = "left", label.y = "bottom", color = "red",
               size = 5)
options(old.option)
```

- Para a variável `Incl`, assim como para `Area`, há evidência de um efeito sobre `PU`.

- Pode-se, assim, ajustar um fator inclinação:

- $F_i = \left (\frac{i_{imovel} + 1}{i_{paradigma} + 1} \right )^{-0,10} = \left (\frac{i_{paradigma} + 1}{i_{imovel} + 1} \right )^{0,10} = \left (\frac{1}{i_{imovel} + 1} \right )^{0,10}$

## Fator inclinação (2) {.smaller}

```{r}
#| echo: true
fitIncl <- lm(log(PU) ~ log1p(Incl), data = dados)
```

```{r}
fitIncl |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2,
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",                        "Inf.", "Sup.")) |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

1. Para o lote paradigma (plano): $F_i = \left( \frac{1}{0+1}\right)^{0,10}=1,0$
2. Para um lote com inclinação igual a 5%: $F_i = \left( \frac{1}{5+1}\right)^{0,10}=0,84$

- O fator também deverá ser aplicado de forma ***multiplicativa***!
- Se o lote paradigma (plano) possui valor igual a `r Reais(exp(coef(fitIncl)["(Intercept)"]))`/m2.
- Então um lote com inclinação de 5% possui VM igual a `r Reais(.84*exp(coef(fitIncl)["(Intercept)"]))`/m2.

. . .

```{r}
#| echo: true
p <- predict(fitIncl, newdata = list(Incl = 5))
exp(p)
```

## Reflexões

- No modelo de regressão linear múltipla, com menos dados disponíveis, o efeito 
da variável `Frente` era positivo, porém estatisticamente insignificante.

- No modelo de regressão simples, com mais dados, o efeito da variável `Frente` 
também se mostrou insignificante, porém negativo.

- Qual o efeito real da variável `Frente`?

## Efeito "real" da variável Frente {.smaller}

- Análise da variável `Frente` *após* a homogeneização com o fator `Area`:

. . .

```{r}
dados <- within(dados, {
  FArea <- (450/Area)^.14
#  FIncl <- (1/(Incl + 1))^.10
  PUhom <- PU/FArea
})
```

```{r}
p2a <-
  ggmice(dados, aes(x = Frente/15, y = PUhom)) +
  geom_point(size = 2) +
  stat_poly_line() +
  # stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/15) + k,
  #             method.args=list(start=c(a=-100, k=10000))) +
  theme_solarized(base_size = 15)
p2a +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P"), sep = "*\"; \"*"), 
               formula = y ~ x, 
               coef.digits = 2, coef.keep.zeros = FALSE, 
               f.digits = 2, p.digits = 2,
               eq.with.lhs = "widehat(ln(PU[hom]))~`=`~",
               eq.x.rhs = ".ln(Frente/15)",
               label.x = "right", label.y = "bottom", color = "red",
               size = 5) +
  scale_y_continuous(transform = "log") +
  scale_x_continuous(transform = "log", breaks = scales::log_breaks()) +
  labs(caption = "Variável Frente centralizada.")
```

## Efeito "real" da variável Área {.smaller}

```{r}
dados <- within(dados, {
  FArea <- (450/Area)^.15
  FFrente <- (Frente/15)^.09
  PUhom2 <- PU/FFrente
})
```

```{r}
p1a <-
  ggmice(dados, aes(x = Area/450, y = PUhom2)) +
  geom_point(size = 2) +
  stat_poly_line() +
  # stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/15) + k,
  #             method.args=list(start=c(a=-100, k=10000))) +
  theme_solarized(base_size = 15)
p1a +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P"), sep = "*\"; \"*"), 
               formula = y ~ x, 
               coef.digits = 2, coef.keep.zeros = FALSE, 
               f.digits = 2, p.digits = 2,
               eq.with.lhs = "widehat(ln(PU[hom]))~`=`~",
               eq.x.rhs = ".ln(Area/450)",
               label.x = "right", label.y = "top", color = "red",
               size = 5) +
  scale_y_continuous(transform = "log") +
  scale_x_continuous(transform = "log", breaks = scales::log_breaks()) +
  labs(caption = "Variável Frente centralizada.")
```

- $R^2$ passou de 0,28 para 0,34! Coeficiente passou de -0,14 para -0,16!


# Correlação Total, Parcial e Semiparcial {background-image="img/PPT_Chapter.png"} 

## Modelo com regressores Area e Frente {.smaller}

```{r}
fit1 <- lm(log(PU) ~ log(Area/450) + log(Frente/15), data = dados)
s1 <- summary(fit1)
fit1 |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2, format.args = list(big.mark = ".", decimal.mark = ","),
        format = "html", 
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",
                      "Inf.", "Sup.")) |>
  footnote(alphabet = c(paste("Dados: ", length(fit1$residuals)),
                        paste("R2: ", 
                              brf(s1$r.squared, digits = 2)),
                        paste("R2aj: ", 
                              brf(s1$adj.r.squared, digits = 2))
                        ),
           escape = F)  |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

- A retirada de `Incl` melhora a estimação dos outros coeficientes (mais dados)!

- A variável `Frente`, agora, demonstrou-se significante e com efeito positivo.

- O coeficiente de determinação diminui bastante com a retirada da variável `Incl`.

## Correlação de Ordem-Zero, Parcial e Semiparcial {.smaller}

- Existem basicamente três tipos de correlação entre variáveis:

  - A de Pearson (ordem-zero), quando analisadas isoladamente.
  - A Parcial, computada enquanto se retira(m) o(s) efeito(s) de outra(s) variável(eis).
  - A semi-parcial, que expressa a relação única entre uma variável independente e a variável dependente.

. . .

::::: columns
::: {.column width="50%"}
![Correlação Parcial](img/Partial.png)
:::

::: {.column width="50%"}
![Correlação Semi-parcial](img/SemiPartial.png)
:::
:::::

## Correlação de Ordem Zero, Parcial e Semi-Parcial (2) {.smaller}

```{r}
olsrr::ols_correlations(fit1) |>
  kable(digits = 2, format.args = list(decimal.mark = ",", big.mark = "."))
```

-   Na tabela acima são vistas a correlação de ordem zero, a parcial e a semi-parcial (coluna *Part*).

-   O valor da correlação semi-parcial elevado ao quadrado é também conhecido como **coeficiente de determinação parcial**!

    -   Por exemplo, para a variável `Frente`: $sr_{Frente}^2 = 0,42^2 \approx 0,18$.

    -   Já para a variável `Area`: $sr_{Area}^2 = -0,54^2 \approx 0,29$

-   O coeficiente de determinação parcial de uma variável representa o percentual de explicação que ela **adiciona** ao modelo!

    -   Por exemplo, a regressão simples da variável `Frente` vs. `PU` tinha $R^2 = 0,04$.
    -   Adicionando a variável `Area` a este modelo, ele ficou com $R^2 = 0,33$!

## Correlação de Ordem Zero, Parcial e Semi-Parcial (3) {.smaller}

```{r}
olsrr::ols_correlations(fit1) |>
  kable(digits = 2, format.args = list(decimal.mark = ",", big.mark = "."))
```

-   A correlação da variável `Area` com relação à PU era fraca ($r = -0,39$)
    -   Porém, após a consideração da variável `Frente`, essa correlação torna-se moderada ($pr = -0,55$)!
-   O mais importante, porém, é perceber que o sinal do coeficiente de correlação parcial muda para a variável `Frente`!
    -   A correlação da variável `Frente` com relação à `PU` era fraca e negativa ($r = -0,19$)
    -   Porém, na presença da variável `Area`, a correlação de `Frente` e `PU` passa a ser positiva e moderada ($pr = 0,45$)!
-   Este efeito, de mudança no sinal da correlação após a consideração de um outro regressor, é denominado de **Paradoxo de Simpson**!

# Paradoxo de Simpson {background-image="img/PPT_Chapter.png"} 

## Correlação entre Área e Frente {.smaller}

```{r}
ggmice(dados, aes(x = Frente/15, y = Area/450)) +
  geom_point() +
  stat_poly_line() +
  # stat_smooth(method="nls", se=FALSE, formula=y~a*log(x) + k,
  #             method.args=list(start=c(a=-100, k=10000))) +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P")), 
               formula = y ~ x, 
               label.x = "left") +
  scale_y_continuous(transform = 'log', breaks = scales::log_breaks()) + 
  scale_x_continuous(transform = 'log') +
  theme_solarized(base_size = 15)
```

-   Existe uma forte correlação entre os regressores!

## Paradoxo de Simpson

-   $\ln(PU) = \beta_0 + \beta_1.\ln(Area/450) + \beta_2.\ln(Frente/15) + \varepsilon_1$

-   $\ln(Area) = \beta_3 + \beta_4.\ln(Frente/15) + \varepsilon_2$

-   $\ln(PU) = \beta_0 + \beta_1\beta_3 + (\beta_1\beta_4 + \beta_2).\ln(Frente/15) + \varepsilon$

-   $\ln(PU) = 7,81 - 0,33.\ln(Area/450) + 0,36\ln(Frente/15) + \varepsilon_1$

-   $\ln(Area/450) = -0,01 + 1,27.\ln(Frente) + \varepsilon_2$

-   $\hat{\ln(PU)} = 7,81 + (-0,33.1,27 + 0,36).\ln(Frente/15)$

-   $\hat{\ln(PU)} = 7,81 - 0,06.\ln(Frente/15)$

-   É por isso que a regressão simples com cada regressor não é, em geral, relevante para o ajuste de fatores de homogeneização!

## Paradoxo de Simpson (2)

```{r}
#| label: mcPlots
#| fig-cap: "Gráficos Marginais/Condicionais"
#| fig-width: 7
#| fig-height: 3.5

library(ggplotify)
library(ggeasy)
library(patchwork)
p1 <- as.ggplot(~mcPlot(fit1, "log(Area/450)", title = FALSE)) + 
  theme_solarized(base_size = 15) +
  easy_remove_axes()
p2 <- as.ggplot(~mcPlot(fit1, "log(Frente/15)", title = FALSE)) + 
  theme_solarized(base_size = 15) +
  easy_remove_axes()
p1 + p2
#p <- as.ggplot(function() (mcPlots(fit1, las = 1, title = FALSE)))
# p + theme_solarized()
```

-   Nos modelos de regressão linear múltipla, o efeito de uma variável é computado após a "homogeneização" da outra!

## Resíduos Parciais

```{r}
plotModel(fit1, residuals = T)
```

## Derivação de Fatores a partir da RLM {.smaller}

```{r}
fit1 |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2, format.args = list(big.mark = ".", decimal.mark = ","),
        format = "html", 
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor",
                      "Inf.", "Sup.")) |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

-   Os fatores derivados devem ser utilizados na forma ***multiplicativa***!

    -   A equação de estimação será: $PU = 2.465,00.F_{Area}.F_{Frente}$

-   O fator Área seria: $F_{Area} = \left ( \frac{450}{A_{imovel}} \right)^{0,33}$

-   O fator Frente seria: $F_{Frente} = \left ( \frac{F_{imovel}}{15} \right)^{0,36}$

## Aplicação do Fator `Incl` {.smaller}

-   Como vimos, é possível ajustarmos bons modelos com a remoção de uma variável que conta com muitos valores faltantes, como a variável `Incl`.

-   No entanto, se a sua remoção do modelo melhora a estimação, por outro lado, o efeito desta variável não pode ser simplesmente ignorado!

-   Uma alternativa é derivar o fator `Incl` a partir de uma regressão simples:

-   Por exemplo, imagine a avaliação de um lote com `Area` = 750m, `Frente` = 25m, porém com 10% de inclinação.

    -   Pode-se aplicar o fator `Incl`: $F_i = \left (\frac{1}{i_{imovel} + 1} \right )^{0,10}$
    -   Assim, o valor de mercado de um lote com `Area` = 750m, `Frente` = 25m e `Incl` igual a 10% é igual a:
    -   $PU = 2.465,00.\left ( \frac{450}{A_{imovel}} \right)^{0,33}.\left ( \frac{F_{imovel}}{15} \right)^{0,36}.\left (\frac{1}{i_{imovel} + 1} \right )^{0,10}$
    -   $PU = 2.465,00.\left ( \frac{450}{750} \right)^{0,33}.\left ( \frac{25}{15} \right)^{0,36}.\left (\frac{1}{10 + 1} \right )^{0,10} \approx$`r Reais(exp(coef(fit1)[1])*.845*1.2*.79)`

# Reflexões {background-image="img/PPT_Chapter.png"} 

## Análise Exploratória

```{r}
library(car)
library(ggplotify)

p <- as.ggplot(function() (
scatterplotMatrix(~ log(PU) + log(Area) + log(Frente) + log(Incl), 
                  data = dados, smooth = FALSE, las = 1)))
p + theme_solarized() +
  easy_remove_axes()
  # theme(axis.text.x = element_blank(),
  #       axis.ticks.x = element_blank(), 
  #       axis.text.y = element_blank(), 
  #       axis.ticks.y = element_blank(),
  #       axis.title.x = element_blank(),
  #       axis.title.y = element_blank()) 
```

## Relação de Inclinação com outros regressores {.smaller}

```{r}
#| label: fig-InclinacaoOthers
#| fig-cap: "Relação da variável Inclinação com outras VE."
#| fig-subcap:
#|   - "Incl vs. Area"
#|   - "Incl vs. Frente"
#| layout-ncol: 2
ggmice(dados, aes(x = Incl, y = Area)) +
  geom_point(size = 2) +
  stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/360) + k,
              method.args=list(start=c(a=-100, k=10000))) +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P")), 
               formula = log(y) ~ log(x), 
               label.x = "right", color = "red") +
  theme_solarized(base_size = 15)
ggmice(dados, aes(x = Incl, y = Frente)) +
  geom_point(size = 2) +
  stat_smooth(method="nls", se=FALSE, formula=y~a*log(x/360) + k,
              method.args=list(start=c(a=-100, k=10000))) +
  stat_poly_eq(use_label(c("eq", "R2", "F", "P")), 
               formula = log(y) ~ log(x), 
               label.x = "right", color = "red") +
  theme_solarized(base_size = 15)
```

-   Não há, na prática, qualquer relação entre `Incl` e as variáveis `Frente` e `Area`.
    -   Isto permite que o fator `Incl`, derivado a partir de uma regressão simples, seja utilizado sem qualquer preocupação a respeito de viés de variável omitida.

## Reflexões {.smaller}

-   No mercado imobilário existem diversas, inúmeras variáveis explicativas

-   Muitas delas estão correlacionadas entre si, a exemplo das variáveis `Area` e `Frente`

    -   Quando as variáveis explicativas estão correlacionadas, não se pode ignorar o efeito da presença/ausência de uma sobre a outra na modelagem

-   Outras variáveis não estão necessariamente correlacionadas com as outras. É o exemplo da variável `Incl`. Podemos também citar a variável `Andar`, em apartamentos, ou a posição no andar

    -   Quando as variáveis não estão potencialmente correlacionadas com as outras variáveis do modelo, faz sentido a derivação de um fator de maneira isolada.
    -   Quando a variável em análise estiver potencialmente correlacionada apenas com algumas das variáveis, pode ser necessário um modelo de regressão múltipla auxiliar com o objetivo de estimar, sem viés, o seu coeficiente.

# Aparte: Imputação de dados {background-image="img/PPT_Chapter.png"} 

## Aparte: imputação de dados {.smaller}

-   Existem algoritmos de imputação de múltipla de dados, como o PMM [@rubin1986], que permitem a imputação de dados faltantes mesmo na presença de não-linearidade ou heteroscedasticidade.

. . .

```{r, include = FALSE}
d <- mice(dados, method = "pmm", m = 1, seed = 20,
     formulas = list("Frente" = log(Frente) ~ log(PU) + log(Area) +
                       log(Incl),
                     "Incl" = log(Incl) ~ log(Frente) + log(PU) +
                       log(Area))
)
```

```{r}
nlsFit <- nls(Area~a*log(Frente) + k, data = dados, start = c(a=-100, k=10000))
FUN <- function(x) coef(nlsFit)["a"]*log(x) + coef(nlsFit)["k"]
ggmice(d, aes(x = Frente, y = Area)) +
  geom_point(size = 2) +
  geom_function(fun = FUN, color = "cornflowerblue", size = 1) +
  theme_solarized(base_size = 15) +
  labs(caption = "Dados imputados através de Correspondência de Média Preditiva.")
```

## Aparte: imputação de dados (2)

```{r}
nlsFit <- nls(PU~a*log(Incl) + k, data = dados, start = c(a=-100, k=10000))
FUN <- function(x) coef(nlsFit)["a"]*log(x) + coef(nlsFit)["k"]
ggmice(d, aes(x = Incl, y = PU)) +
  geom_point(size = 2) +
  geom_function(fun = FUN, color = "cornflowerblue", size = 1) +
  theme_solarized(base_size = 15) +
  labs(caption = "Dados imputados através de Correspondência de Média Preditiva.")
```

## Aparte: imputação de dados (3) {.smaller}

### Modelo com dados imputados

```{r}
ImpFit <- lm(log(PU) ~ log(Area/450) + log(Frente/15) + log1p(Incl), 
             data = complete(d))
sImp <- summary(ImpFit)
ImpFit |>
  tidy(conf.int = T, conf.level = .80) |>
  kable(digits = 2, format.args = list(big.mark = ".", decimal.mark = ","),
        format = "html",
        col.names = c("Termo", "Est.", "Erro", "Est. t", "p-valor", 
                      "Inf.", "Sup.")) |>
  footnote(alphabet = c(paste("Dados: ", length(ImpFit$residuals)),
                        paste("R2: ", 
                              brf(sImp$r.squared, digits = 2)),
                        paste("R2aj: ", 
                              brf(sImp$adj.r.squared, digits = 2))
                        ),
           escape = F)  |>
  add_header_above(c(" " = 5, "IC (80%)" = 2))
```

-   A imputação de dados permite o ajuste de modelos com todas as variáveis, aproveitando todas as informações disponíveis.

-   Este modelo pode ser utilizado, posteriormente, para o ajuste de fatores.

## Aparte: imputação de dados (4) {.smaller}

-   O procedimento de imputação de dados poderia ser padronizado pela NBR 14.653, visando permitir imputações de dados, desde que baseados em procedimentos pré-definidos!

-   @doi:10.1080/02664763.2016.1158246: Melhor método para imputação de dados em pequenas amostras: *joint multiple imputation* (JMI)!

. . .

```{r}
library(JointAI)
dados <- within(dados,{
  logPU <- log(PU)
  logArea <- log(Area/450)
  logFrente <- log(Frente/15)
  logIncl <- log1p(Incl)
})
ImpFit1 <- lm_imp(logPU ~ logArea + logFrente + logIncl, data = dados, 
                  n.iter = 1000, seed = 1)
summary(ImpFit1)
```

## Referências
